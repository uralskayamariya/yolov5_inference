{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инференс моделей PyTorch 'pt' (yolov5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо * необходимо подставить свои значения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Из репозитория"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/ultralytics/yolov5\n",
    "%cd yolov5\n",
    "!python detect.py \\\n",
    "\t--weights best.pt \\\n",
    "    --source *.png \\\n",
    "    --conf-thres 0.001 \\\n",
    "    --iou-thres 0.45 \\\n",
    "\t--img 640 \\\n",
    "    --project project_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты сохраняются по адресу: project_path\\exp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Средствами PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка модели\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='*.pt', force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoShape(\n",
       "  (model): DetectMultiBackend(\n",
       "    (model): DetectionModel(\n",
       "      (model): Sequential(\n",
       "        (0): Conv(\n",
       "          (conv): Conv2d(3, 32, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv(\n",
       "          (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (2): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Conv(\n",
       "          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (4): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): Conv(\n",
       "          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (6): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): Conv(\n",
       "          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (8): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): SPPF(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (10): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (11): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (12): Concat()\n",
       "        (13): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (14): Conv(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (15): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (16): Concat()\n",
       "        (17): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (18): Conv(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (19): Concat()\n",
       "        (20): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (21): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (22): Concat()\n",
       "        (23): C3(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv3): Conv(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (m): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (cv1): Conv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "              (cv2): Conv(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (act): SiLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (24): Detect(\n",
       "          (m): ModuleList(\n",
       "            (0): Conv2d(128, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): Conv2d(256, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (2): Conv2d(512, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# слои модели\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.model.model.0.conv.weight: torch.Size([32, 3, 6, 6])\n",
      "model.model.model.0.conv.bias: torch.Size([32])\n",
      "model.model.model.1.conv.weight: torch.Size([64, 32, 3, 3])\n",
      "model.model.model.1.conv.bias: torch.Size([64])\n",
      "model.model.model.2.cv1.conv.weight: torch.Size([32, 64, 1, 1])\n",
      "model.model.model.2.cv1.conv.bias: torch.Size([32])\n",
      "model.model.model.2.cv2.conv.weight: torch.Size([32, 64, 1, 1])\n",
      "model.model.model.2.cv2.conv.bias: torch.Size([32])\n",
      "model.model.model.2.cv3.conv.weight: torch.Size([64, 64, 1, 1])\n",
      "model.model.model.2.cv3.conv.bias: torch.Size([64])\n",
      "model.model.model.2.m.0.cv1.conv.weight: torch.Size([32, 32, 1, 1])\n",
      "model.model.model.2.m.0.cv1.conv.bias: torch.Size([32])\n",
      "model.model.model.2.m.0.cv2.conv.weight: torch.Size([32, 32, 3, 3])\n",
      "model.model.model.2.m.0.cv2.conv.bias: torch.Size([32])\n",
      "model.model.model.3.conv.weight: torch.Size([128, 64, 3, 3])\n",
      "model.model.model.3.conv.bias: torch.Size([128])\n",
      "model.model.model.4.cv1.conv.weight: torch.Size([64, 128, 1, 1])\n",
      "model.model.model.4.cv1.conv.bias: torch.Size([64])\n",
      "model.model.model.4.cv2.conv.weight: torch.Size([64, 128, 1, 1])\n",
      "model.model.model.4.cv2.conv.bias: torch.Size([64])\n",
      "model.model.model.4.cv3.conv.weight: torch.Size([128, 128, 1, 1])\n",
      "model.model.model.4.cv3.conv.bias: torch.Size([128])\n",
      "model.model.model.4.m.0.cv1.conv.weight: torch.Size([64, 64, 1, 1])\n",
      "model.model.model.4.m.0.cv1.conv.bias: torch.Size([64])\n",
      "model.model.model.4.m.0.cv2.conv.weight: torch.Size([64, 64, 3, 3])\n",
      "model.model.model.4.m.0.cv2.conv.bias: torch.Size([64])\n",
      "model.model.model.4.m.1.cv1.conv.weight: torch.Size([64, 64, 1, 1])\n",
      "model.model.model.4.m.1.cv1.conv.bias: torch.Size([64])\n",
      "model.model.model.4.m.1.cv2.conv.weight: torch.Size([64, 64, 3, 3])\n",
      "model.model.model.4.m.1.cv2.conv.bias: torch.Size([64])\n",
      "model.model.model.5.conv.weight: torch.Size([256, 128, 3, 3])\n",
      "model.model.model.5.conv.bias: torch.Size([256])\n",
      "model.model.model.6.cv1.conv.weight: torch.Size([128, 256, 1, 1])\n",
      "model.model.model.6.cv1.conv.bias: torch.Size([128])\n",
      "model.model.model.6.cv2.conv.weight: torch.Size([128, 256, 1, 1])\n",
      "model.model.model.6.cv2.conv.bias: torch.Size([128])\n",
      "model.model.model.6.cv3.conv.weight: torch.Size([256, 256, 1, 1])\n",
      "model.model.model.6.cv3.conv.bias: torch.Size([256])\n",
      "model.model.model.6.m.0.cv1.conv.weight: torch.Size([128, 128, 1, 1])\n",
      "model.model.model.6.m.0.cv1.conv.bias: torch.Size([128])\n",
      "model.model.model.6.m.0.cv2.conv.weight: torch.Size([128, 128, 3, 3])\n",
      "model.model.model.6.m.0.cv2.conv.bias: torch.Size([128])\n",
      "model.model.model.6.m.1.cv1.conv.weight: torch.Size([128, 128, 1, 1])\n",
      "model.model.model.6.m.1.cv1.conv.bias: torch.Size([128])\n",
      "model.model.model.6.m.1.cv2.conv.weight: torch.Size([128, 128, 3, 3])\n",
      "model.model.model.6.m.1.cv2.conv.bias: torch.Size([128])\n",
      "model.model.model.6.m.2.cv1.conv.weight: torch.Size([128, 128, 1, 1])\n",
      "model.model.model.6.m.2.cv1.conv.bias: torch.Size([128])\n",
      "model.model.model.6.m.2.cv2.conv.weight: torch.Size([128, 128, 3, 3])\n",
      "model.model.model.6.m.2.cv2.conv.bias: torch.Size([128])\n",
      "model.model.model.7.conv.weight: torch.Size([512, 256, 3, 3])\n",
      "model.model.model.7.conv.bias: torch.Size([512])\n",
      "model.model.model.8.cv1.conv.weight: torch.Size([256, 512, 1, 1])\n",
      "model.model.model.8.cv1.conv.bias: torch.Size([256])\n",
      "model.model.model.8.cv2.conv.weight: torch.Size([256, 512, 1, 1])\n",
      "model.model.model.8.cv2.conv.bias: torch.Size([256])\n",
      "model.model.model.8.cv3.conv.weight: torch.Size([512, 512, 1, 1])\n",
      "model.model.model.8.cv3.conv.bias: torch.Size([512])\n",
      "model.model.model.8.m.0.cv1.conv.weight: torch.Size([256, 256, 1, 1])\n",
      "model.model.model.8.m.0.cv1.conv.bias: torch.Size([256])\n",
      "model.model.model.8.m.0.cv2.conv.weight: torch.Size([256, 256, 3, 3])\n",
      "model.model.model.8.m.0.cv2.conv.bias: torch.Size([256])\n",
      "model.model.model.9.cv1.conv.weight: torch.Size([256, 512, 1, 1])\n",
      "model.model.model.9.cv1.conv.bias: torch.Size([256])\n",
      "model.model.model.9.cv2.conv.weight: torch.Size([512, 1024, 1, 1])\n",
      "model.model.model.9.cv2.conv.bias: torch.Size([512])\n",
      "model.model.model.10.conv.weight: torch.Size([256, 512, 1, 1])\n",
      "model.model.model.10.conv.bias: torch.Size([256])\n",
      "model.model.model.13.cv1.conv.weight: torch.Size([128, 512, 1, 1])\n",
      "model.model.model.13.cv1.conv.bias: torch.Size([128])\n",
      "model.model.model.13.cv2.conv.weight: torch.Size([128, 512, 1, 1])\n",
      "model.model.model.13.cv2.conv.bias: torch.Size([128])\n",
      "model.model.model.13.cv3.conv.weight: torch.Size([256, 256, 1, 1])\n",
      "model.model.model.13.cv3.conv.bias: torch.Size([256])\n",
      "model.model.model.13.m.0.cv1.conv.weight: torch.Size([128, 128, 1, 1])\n",
      "model.model.model.13.m.0.cv1.conv.bias: torch.Size([128])\n",
      "model.model.model.13.m.0.cv2.conv.weight: torch.Size([128, 128, 3, 3])\n",
      "model.model.model.13.m.0.cv2.conv.bias: torch.Size([128])\n",
      "model.model.model.14.conv.weight: torch.Size([128, 256, 1, 1])\n",
      "model.model.model.14.conv.bias: torch.Size([128])\n",
      "model.model.model.17.cv1.conv.weight: torch.Size([64, 256, 1, 1])\n",
      "model.model.model.17.cv1.conv.bias: torch.Size([64])\n",
      "model.model.model.17.cv2.conv.weight: torch.Size([64, 256, 1, 1])\n",
      "model.model.model.17.cv2.conv.bias: torch.Size([64])\n",
      "model.model.model.17.cv3.conv.weight: torch.Size([128, 128, 1, 1])\n",
      "model.model.model.17.cv3.conv.bias: torch.Size([128])\n",
      "model.model.model.17.m.0.cv1.conv.weight: torch.Size([64, 64, 1, 1])\n",
      "model.model.model.17.m.0.cv1.conv.bias: torch.Size([64])\n",
      "model.model.model.17.m.0.cv2.conv.weight: torch.Size([64, 64, 3, 3])\n",
      "model.model.model.17.m.0.cv2.conv.bias: torch.Size([64])\n",
      "model.model.model.18.conv.weight: torch.Size([128, 128, 3, 3])\n",
      "model.model.model.18.conv.bias: torch.Size([128])\n",
      "model.model.model.20.cv1.conv.weight: torch.Size([128, 256, 1, 1])\n",
      "model.model.model.20.cv1.conv.bias: torch.Size([128])\n",
      "model.model.model.20.cv2.conv.weight: torch.Size([128, 256, 1, 1])\n",
      "model.model.model.20.cv2.conv.bias: torch.Size([128])\n",
      "model.model.model.20.cv3.conv.weight: torch.Size([256, 256, 1, 1])\n",
      "model.model.model.20.cv3.conv.bias: torch.Size([256])\n",
      "model.model.model.20.m.0.cv1.conv.weight: torch.Size([128, 128, 1, 1])\n",
      "model.model.model.20.m.0.cv1.conv.bias: torch.Size([128])\n",
      "model.model.model.20.m.0.cv2.conv.weight: torch.Size([128, 128, 3, 3])\n",
      "model.model.model.20.m.0.cv2.conv.bias: torch.Size([128])\n",
      "model.model.model.21.conv.weight: torch.Size([256, 256, 3, 3])\n",
      "model.model.model.21.conv.bias: torch.Size([256])\n",
      "model.model.model.23.cv1.conv.weight: torch.Size([256, 512, 1, 1])\n",
      "model.model.model.23.cv1.conv.bias: torch.Size([256])\n",
      "model.model.model.23.cv2.conv.weight: torch.Size([256, 512, 1, 1])\n",
      "model.model.model.23.cv2.conv.bias: torch.Size([256])\n",
      "model.model.model.23.cv3.conv.weight: torch.Size([512, 512, 1, 1])\n",
      "model.model.model.23.cv3.conv.bias: torch.Size([512])\n",
      "model.model.model.23.m.0.cv1.conv.weight: torch.Size([256, 256, 1, 1])\n",
      "model.model.model.23.m.0.cv1.conv.bias: torch.Size([256])\n",
      "model.model.model.23.m.0.cv2.conv.weight: torch.Size([256, 256, 3, 3])\n",
      "model.model.model.23.m.0.cv2.conv.bias: torch.Size([256])\n",
      "model.model.model.24.m.0.weight: torch.Size([18, 128, 1, 1])\n",
      "model.model.model.24.m.0.bias: torch.Size([18])\n",
      "model.model.model.24.m.1.weight: torch.Size([18, 256, 1, 1])\n",
      "model.model.model.24.m.1.bias: torch.Size([18])\n",
      "model.model.model.24.m.2.weight: torch.Size([18, 512, 1, 1])\n",
      "model.model.model.24.m.2.bias: torch.Size([18])\n"
     ]
    }
   ],
   "source": [
    "# так удобнее посмотреть слои\n",
    "for name, para in model.named_parameters():\n",
    "    print('{}: {}'.format(name, para.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инференс\n",
    "img = '*.png'\n",
    "results = model(img)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(results.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>confidence</th>\n",
       "      <th>class</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>412.377747</td>\n",
       "      <td>242.831482</td>\n",
       "      <td>435.799500</td>\n",
       "      <td>295.981079</td>\n",
       "      <td>0.633974</td>\n",
       "      <td>0</td>\n",
       "      <td>Corrosion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>176.143661</td>\n",
       "      <td>396.686401</td>\n",
       "      <td>214.988174</td>\n",
       "      <td>508.654114</td>\n",
       "      <td>0.617267</td>\n",
       "      <td>0</td>\n",
       "      <td>Corrosion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>403.028503</td>\n",
       "      <td>11.713810</td>\n",
       "      <td>427.832153</td>\n",
       "      <td>76.807175</td>\n",
       "      <td>0.303495</td>\n",
       "      <td>0</td>\n",
       "      <td>Corrosion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>403.225586</td>\n",
       "      <td>578.997864</td>\n",
       "      <td>432.352905</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>0.251476</td>\n",
       "      <td>0</td>\n",
       "      <td>Corrosion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>404.885620</td>\n",
       "      <td>54.259666</td>\n",
       "      <td>427.078186</td>\n",
       "      <td>97.550575</td>\n",
       "      <td>0.251334</td>\n",
       "      <td>0</td>\n",
       "      <td>Corrosion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         xmin        ymin        xmax        ymax  confidence  class  \\\n",
       "0  412.377747  242.831482  435.799500  295.981079    0.633974      0   \n",
       "1  176.143661  396.686401  214.988174  508.654114    0.617267      0   \n",
       "2  403.028503   11.713810  427.832153   76.807175    0.303495      0   \n",
       "3  403.225586  578.997864  432.352905  640.000000    0.251476      0   \n",
       "4  404.885620   54.259666  427.078186   97.550575    0.251334      0   \n",
       "\n",
       "        name  \n",
       "0  Corrosion  \n",
       "1  Corrosion  \n",
       "2  Corrosion  \n",
       "3  Corrosion  \n",
       "4  Corrosion  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pandas().xyxy[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cохранение модели yolov5 в onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/ultralytics/yolov5\n",
    "%cd yolov5\n",
    "!python export.py \\\n",
    "\t--weights *.pt \\\n",
    "\t--img 320 \\\n",
    "\t--batch 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инференс моделей onnx (yolov5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# определите параметры детектирования\n",
    "INPUT_WIDTH = 640 # ширина входных изображений\n",
    "INPUT_HEIGHT = 640 # высота входных изображений\n",
    "SCORE_THRESHOLD = 0.45 # порог уверенности модели классификации\n",
    "NMS_THRESHOLD = 0.45 # порог площади перекрытия для исключения лишних рамок\n",
    "CONFIDENCE_THRESHOLD = 0.5 # порог уверенности модели детектора\n",
    "\n",
    "# укажите путь загрузки файла имен классов\n",
    "classesFile = 'names.names'\n",
    "# укажите путь к изображению\n",
    "frame = cv2.imread('*.png')\n",
    "# укажите путь к модели onnx\n",
    "modelWeights = '*.onnx'\n",
    "\n",
    "# Text parameters\n",
    "FONT_FACE = cv2.FONT_HERSHEY_SIMPLEX\n",
    "FONT_SCALE = 0.5\n",
    "THICKNESS = 1\n",
    "# Colors\n",
    "BLACK  = (0,0,0)\n",
    "BLUE   = (255,178,50)\n",
    "YELLOW = (0,255,255)\n",
    "\n",
    "\n",
    "def draw_label(im, label, x, y):\n",
    "    \"\"\"Draw text onto image at location.\"\"\"\n",
    "    cv2.putText(im, label, (x, y), FONT_FACE, FONT_SCALE, BLACK, THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "def pre_process(input_image, net):\n",
    "      # Create a 4D blob from a frame.\n",
    "      blob = cv2.dnn.blobFromImage(input_image, 1/255,  (INPUT_WIDTH, INPUT_HEIGHT), [0,0,0], 1, crop=False)\n",
    "      # Sets the input to the network.\n",
    "      net.setInput(blob)\n",
    "      # Run the forward pass to get output of the output layers.\n",
    "      outputs = net.forward(net.getUnconnectedOutLayersNames())\n",
    "\n",
    "      return outputs\n",
    "\n",
    "\n",
    "def post_process(input_image, outputs):\n",
    "      # Lists to hold respective values while unwrapping.\n",
    "      class_ids = []\n",
    "      confidences = []\n",
    "      boxes = []\n",
    "      # Rows.\n",
    "      rows = outputs[0].shape[1]\n",
    "      image_height, image_width = input_image.shape[:2]\n",
    "      # Resizing factor.\n",
    "      x_factor = image_width / INPUT_WIDTH\n",
    "      y_factor =  image_height / INPUT_HEIGHT\n",
    "      # Iterate through detections\n",
    "      for r in range(rows):\n",
    "            row = outputs[0][0][r]\n",
    "            confidence = row[4]\n",
    "            # Discard bad detections and continue.\n",
    "            if confidence >= CONFIDENCE_THRESHOLD:\n",
    "                  classes_scores = row[5:]\n",
    "                  # Get the index of max class score.\n",
    "                  class_id = np.argmax(classes_scores)\n",
    "                  #  Continue if the class score is above threshold.\n",
    "                  if (classes_scores[class_id] > SCORE_THRESHOLD):\n",
    "                        confidences.append(confidence)\n",
    "                        class_ids.append(class_id)\n",
    "                        cx, cy, w, h = row[0], row[1], row[2], row[3]\n",
    "                        left = int((cx - w/2) * x_factor)\n",
    "                        top = int((cy - h/2) * y_factor)\n",
    "                        width = int(w * x_factor)\n",
    "                        height = int(h * y_factor)\n",
    "                        box = np.array([left, top, width, height])\n",
    "                        boxes.append(box)\n",
    "\n",
    "      # Perform non maximum suppression to eliminate redundant, overlapping boxes with lower confidences.\n",
    "      indices = cv2.dnn.NMSBoxes(boxes, confidences, CONFIDENCE_THRESHOLD, NMS_THRESHOLD)\n",
    "      for i in indices:\n",
    "            box = boxes[i]\n",
    "            left = box[0]\n",
    "            top = box[1]\n",
    "            width = box[2]\n",
    "            height = box[3]             \n",
    "            # Draw bounding box.             \n",
    "            cv2.rectangle(input_image, (left, top), (left + width, top + height), BLACK, 3*THICKNESS)\n",
    "            # Class label.                      \n",
    "            label = \"{}:{:.2f}\".format(classes[class_ids[i]], confidences[i])             \n",
    "            # Draw label.             \n",
    "            draw_label(input_image, label, left, top)\n",
    "\n",
    "            list_left.append(left)\n",
    "            list_top.append(top)\n",
    "            list_width.append(width)\n",
    "            list_height.append(height)\n",
    "            list_conf.append(confidences[i])\n",
    "            list_class.append(classes[class_ids[i]])\n",
    "\n",
    "      df_det_post = pd.DataFrame({'left': list_left, 'top': list_top, 'width': list_width, 'height': list_height, 'conf': list_conf, 'classes': list_class})\n",
    "      print(df_det_post)\n",
    "\n",
    "      # сохраним детекции в текстовый файл\n",
    "      try:\n",
    "            os.remove(f\"*.txt\")\n",
    "      except:\n",
    "            pass\n",
    "      df_det_post.to_csv(\"*.txt\", header=None, index=None, sep=' ', mode='a')\n",
    "      print(\"Детекции сохранены: *.txt\")\n",
    "\n",
    "      return input_image\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "      classes = None\n",
    "      with open(classesFile, 'rt') as f:\n",
    "            classes = f.read().rstrip('\\n').split('\\n')\n",
    "      \n",
    "      net = cv2.dnn.readNet(modelWeights)\n",
    "\n",
    "      detections = pre_process(frame, net)\n",
    "\n",
    "      list_left = []\n",
    "      list_top = []\n",
    "      list_width = []\n",
    "      list_height = []\n",
    "      list_conf = []\n",
    "      list_class = []\n",
    "      img = post_process(frame.copy(), detections)\n",
    "      \n",
    "      # вычисление времени инференса\n",
    "      t, _ = net.getPerfProfile()\n",
    "      label = 'Inference time: %.2f ms' % (t * 1000.0 /  cv2.getTickFrequency())\n",
    "      print(label)\n",
    "\n",
    "      # сохраним изображение с результатами детектирования\n",
    "      cv2.putText(img, label, (20, 40), FONT_FACE, FONT_SCALE,  (0, 0, 255), THICKNESS, cv2.LINE_AA)\n",
    "      cv2.imwrite('*.png', img)\n",
    "      print('Результат выполнения кода на изображении можно посмотреть здесь: *.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему-то этот код не сработал при тестировании на другом компьютере.\n",
    "\n",
    "Зато работает следующий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import onnxruntime as onnxrt\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Constants\n",
    "INPUT_WIDTH = 640 # ширина входных изображений\n",
    "INPUT_HEIGHT = 640 # высота входных изображений\n",
    "SCORE_THRESHOLD = 0.45 # порог уверенности модели классификации\n",
    "NMS_THRESHOLD = 0.45 # порог площади перекрытия для исключения лишних рамок\n",
    "CONFIDENCE_THRESHOLD = 0.001 # порог уверенности модели детектора\n",
    "\n",
    "# Text parameters.\n",
    "FONT_FACE = cv2.FONT_HERSHEY_SIMPLEX\n",
    "FONT_SCALE = 0.5\n",
    "THICKNESS = 1\n",
    "\n",
    "# Colors.\n",
    "BLACK  = (0,0,0)\n",
    "BLUE   = (255,178,50)\n",
    "YELLOW = (0,255,255)\n",
    "\n",
    "\n",
    "def draw_label(im, label, x, y):\n",
    "    \"\"\"Draw text onto image at location.\"\"\"\n",
    "    cv2.putText(im, label, (x, y), FONT_FACE, FONT_SCALE, BLACK, THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "def pre_process(input_image, onnx_session):\n",
    "      start = time.time()\n",
    "      input_image = cv2.dnn.blobFromImage(input_image, 1/255,  (INPUT_WIDTH, INPUT_HEIGHT), [0,0,0], 1, crop=False)\n",
    "      onnx_inputs = {onnx_session.get_inputs()[0].name:input_image.astype(np.float32)}\n",
    "      onnx_output = onnx_session.run(None, onnx_inputs)\n",
    "      t = time.time() - start\n",
    "\n",
    "      return onnx_output, t\n",
    "\n",
    "\n",
    "def post_process(input_image, outputs):\n",
    "    # Lists to hold respective values while unwrapping.\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    # Rows.\n",
    "    rows = outputs[0].shape[1]\n",
    "    image_height, image_width = input_image.shape[:2]\n",
    "    # Resizing factor.\n",
    "    x_factor = image_width / INPUT_WIDTH\n",
    "    y_factor =  image_height / INPUT_HEIGHT\n",
    "    # Iterate through detections.\n",
    "    for r in range(rows):\n",
    "        row = outputs[0][0][r]\n",
    "        confidence = row[4]\n",
    "        # Discard bad detections and continue.\n",
    "        if confidence >= CONFIDENCE_THRESHOLD:\n",
    "            classes_scores = row[5:]\n",
    "            # Get the index of max class score.\n",
    "            class_id = np.argmax(classes_scores)\n",
    "            #  Continue if the class score is above threshold.\n",
    "            if (classes_scores[class_id] > SCORE_THRESHOLD):\n",
    "                confidences.append(confidence)\n",
    "                class_ids.append(class_id)\n",
    "                cx, cy, w, h = row[0], row[1], row[2], row[3]\n",
    "                left = int((cx - w/2) * x_factor)\n",
    "                top = int((cy - h/2) * y_factor)\n",
    "                width = int(w * x_factor)\n",
    "                height = int(h * y_factor)\n",
    "                box = np.array([left, top, width, height])\n",
    "                boxes.append(box)\n",
    "\n",
    "    # Perform non maximum suppression to eliminate redundant, overlapping boxes with lower confidences.\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, CONFIDENCE_THRESHOLD, NMS_THRESHOLD)\n",
    "    for i in indices:\n",
    "        box = boxes[i]\n",
    "        left = box[0]\n",
    "        top = box[1]\n",
    "        width = box[2]\n",
    "        height = box[3]             \n",
    "        # Draw bounding box.             \n",
    "        cv2.rectangle(input_image, (left, top), (left + width, top + height), BLACK, 3*THICKNESS)\n",
    "        # Class label.                      \n",
    "        label = \"{}:{:.2f}\".format(classes[class_ids[i]], confidences[i])             \n",
    "        # Draw label.             \n",
    "        draw_label(input_image, label, left, top)\n",
    "\n",
    "        list_left.append(left)\n",
    "        list_top.append(top)\n",
    "        list_width.append(width)\n",
    "        list_height.append(height)\n",
    "        list_conf.append(confidences[i])\n",
    "        list_class.append(classes[class_ids[i]])\n",
    "\n",
    "    df_det_post = pd.DataFrame({'left': list_left, 'top': list_top, 'width': list_width, 'height': list_height, 'conf': list_conf, 'classes': list_class})\n",
    "    print(df_det_post)\n",
    "\n",
    "    # сохраним детекции в текстовый файл\n",
    "    try:\n",
    "        os.remove(f\"{folder_to_save}/{name.split('.')[0]}.txt\")\n",
    "    except:\n",
    "        pass\n",
    "    df_det_post.to_csv(f\"{folder_to_save}/{name.split('.')[0]}.txt\", header=None, index=None, sep=' ', mode='a')\n",
    "    folder = folder_to_save.replace('/', '\\\\')\n",
    "    print(f\"Детекции сохранены здесь: {folder}\\{name.split('.')[0]}.txt\")\n",
    "\n",
    "    return input_image\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # имена классов\n",
    "    classesFile = 'names.names'\n",
    "    classes = None\n",
    "    with open(classesFile, 'rt') as f:\n",
    "        classes = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "    # путь для сохранения результатов\n",
    "    folder_to_save = '*'\n",
    "    name = '*.png'\n",
    "    \n",
    "    # путь к изображению\n",
    "    img_path = '*.png'\n",
    "    frame = cv2.imread(img_path)\n",
    "\n",
    "    onnx_session= onnxrt.InferenceSession(\"*.onnx\")\n",
    "\n",
    "    detections, t = pre_process(frame, onnx_session)\n",
    "    \n",
    "    list_left = []\n",
    "    list_top = []\n",
    "    list_width = []\n",
    "    list_height = []\n",
    "    list_conf = []\n",
    "    list_class = []\n",
    "    img = post_process(frame.copy(), detections)\n",
    "      \n",
    "    # вычисление времени инференса\n",
    "    label = 'Inference time: %.2f ms' % (t * 1000)\n",
    "    print(label)\n",
    "\n",
    "    # сохраним изображение с результатами детектирования\n",
    "    cv2.putText(img, label, (20, 40), FONT_FACE, FONT_SCALE,  (0, 0, 255), THICKNESS, cv2.LINE_AA)\n",
    "    cv2.imwrite(f'{folder_to_save}/{name}', img)\n",
    "    folder = folder_to_save.replace('/', '\\\\')\n",
    "    print(f\"Результат выполнения кода на изображении можно посмотреть здесь: {folder}\\{name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Из репозитория"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/ultralytics/yolov5\n",
    "%cd yolov5\n",
    "!python detect.py \\\n",
    "\t--weights *.onnx \\\n",
    "    --source *.png \\\n",
    "    --conf-thres 0.25 \\\n",
    "    --iou-thres 0.45 \\\n",
    "\t--img 640 \\\n",
    "    --project project_path \\\n",
    "    --save-txt \\\n",
    "    --save-conf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torchgpu1101')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01a05369079c257356c581828894a2787192c230d0151bef954786e98d8d305e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
